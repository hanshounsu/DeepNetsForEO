{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanshounsu/DeepNetsForEO/blob/master/nb/Spark_TTS_(0_5B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xKgNTKiHZ_F"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v0daxqKHZ_H"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDnadAgHHZ_I"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3GyRTGRHZ_I"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6dsvDRFgHZ_I"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!git clone https://github.com/SparkAudio/Spark-TTS\n",
        "!pip install omegaconf einx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "execution": {
          "iopub.execute_input": "2025-03-22T00:48:54.511089Z",
          "iopub.status.busy": "2025-03-22T00:48:54.510770Z",
          "iopub.status.idle": "2025-03-22T00:51:37.363415Z",
          "shell.execute_reply": "2025-03-22T00:51:37.362696Z",
          "shell.execute_reply.started": "2025-03-22T00:48:54.511053Z"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "f85fec4d-0f65-4b8e-f45e-f6101cbc38c5",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-311422711.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unexpected optimization option combo_kernels, known options are ['TYPE_CHECKING', 'debug', 'disable_progress', 'verbose_progress', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache', 'force_disable_caches', 'cpp_wrapper', 'abi_compatible', 'c_shim_version', 'dce', 'static_weight_shapes', 'size_asserts', 'nan_asserts', 'pick_loop_orders', 'inplace_buffers', 'allow_buffer_reuse', 'memory_planning', 'memory_pool', 'benchmark_harness', 'epilogue_fusion', 'epilogue_fusion_first', 'pattern_matcher', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'split_cat_fx_passes', 'efficient_conv_bn_eval_fx_passes', 'is_predispatch', 'group_fusion', 'batch_fusion', 'pre_grad_fusion_options', 'post_grad_fusion_options', 'reorder_for_locality', 'dynamic_scale_rblock', 'force_fuse_int_mm_with_mul', 'use_mixed_mm', 'fx_passes_numeric_check', 'force_mixed_mm', 'reorder_for_compute_comm_overlap', 'reorder_for_compute_comm_overlap_passes', 'estimate_op_runtime', 'intra_node_bw', 'inter_node_bw', 'max_autotune', 'max_autotune_pointwise', 'max_autotune_gemm', 'force_same_precision', 'max_autotune_gemm_backends', 'max_autotune_gemm_search_space', 'autotune_fallback_to_aten', 'unbacked_symint_fallback', 'search_autotune_cache', 'save_args', 'autotune_in_subproc', 'max_autotune_subproc_result_timeout_seconds', 'max_autotune_subproc_graceful_timeout_seconds', 'max_autotune_subproc_terminate_timeout_seconds', 'autotune_multi_device', 'coordinate_descent_tuning', 'coordinate_descent_check_all_directions', 'coordinate_descent_search_radius', 'layout_opt_default', 'layout_optimization', 'force_layout_optimization', 'keep_output_stride', 'warn_mix_layout', 'realize_reads_threshold', 'realize_opcount_threshold', 'realize_acc_reads_threshold', 'fallback_random', 'implicit_fallbacks', 'aggressive_fusion', 'debug_fusion', 'benchmark_fusion', 'enabled_metric_tables', 'benchmark_epilogue_fusion', 'max_epilogue_benchmarked_choices', 'max_fusion_size', 'max_pointwise_cat_inputs', 'unroll_reductions_threshold', 'comment_origin', 'conv_1x1_as_mm', 'split_reductions', 'benchmark_kernel', 'constant_and_index_propagation', 'always_keep_tensor_constants', 'assert_indirect_indexing', 'compute_all_bounds', 'joint_graph_constant_folding', 'debug_index_asserts', 'is_nightly_or_source', 'developer_warnings', 'worker_start_method', '_fuse_ddp_communication', '_fuse_ddp_bucket_size', '_fuse_ddp_communication_passes', '_micro_pipeline_tp', 'compile_threads', 'global_cache_dir', 'kernel_name_max_ops', 'shape_padding', 'comprehensive_padding', 'pad_channels_last', 'bw_outputs_user_visible', 'force_shape_pad', 'permute_fusion', 'profiler_mark_wrapper_call', 'generate_intermediate_hooks', 'debug_ir_traceback', '_raise_error_for_testing', '_profile_var', 'profile_bandwidth', 'profile_bandwidth_regex', 'profile_bandwidth_output', 'disable_cpp_codegen', 'freezing', 'freezing_discard_parameters', 'allow_stack_allocation', 'use_minimal_arrayref_interface', 'decompose_mem_bound_mm', 'assume_aligned_inputs', 'cpp.threads', 'cpp.no_redundant_loops', 'cpp.dynamic_threads', 'cpp.simdlen', 'cpp.min_chunk_size', 'cpp.cxx', 'cpp.enable_kernel_profile', 'cpp.weight_prepack', 'cpp.inject_relu_bug_TESTING_ONLY', 'cpp.inject_log1p_bug_TESTING_ONLY', 'cpp.vec_isa_ok', 'cpp.descriptive_names', 'cpp.max_horizontal_fusion_size', 'cpp.fallback_scatter_reduce_sum', 'cpp.enable_unsafe_math_opt_flag', 'cpp.enable_floating_point_contract_flag', 'triton.cudagraphs', 'triton.cudagraph_trees', 'triton.cudagraph_skip_dynamic_graphs', 'triton.slow_path_cudagraph_asserts', 'triton.cudagraph_trees_history_recording', 'triton.cudagraph_support_input_mutation', 'triton.force_cudagraph_sync', 'triton.force_cudagraphs_warmup', 'triton.fast_path_cudagraph_asserts', 'triton.skip_cudagraph_warmup', 'triton.debug_sync_graph', 'triton.debug_sync_kernel', 'triton.dense_indexing', 'triton.max_tiles', 'triton.autotune_pointwise', 'triton.autotune_cublasLt', 'triton.tiling_prevents_pointwise_fusion', 'triton.tiling_prevents_reduction_fusion', 'triton.unique_kernel_names', 'triton.descriptive_names', 'triton.persistent_reductions', 'triton.multi_kernel', 'triton.divisible_by_16', 'triton.min_split_scan_rblock', 'triton.store_cubin', 'triton.spill_threshold', 'triton.use_block_ptr', 'triton.inject_relu_bug_TESTING_ONLY', 'aot_inductor.output_path', 'aot_inductor.debug_compile', 'aot_inductor.debug_dump_consts_bin', 'aot_inductor.serialized_in_spec', 'aot_inductor.serialized_out_spec', 'aot_inductor.use_runtime_constant_folding', 'aot_inductor.force_mmap_weights', 'cuda.arch', 'cuda.version', 'cuda.compile_opt_level', 'cuda.enable_cuda_lto', 'cuda.enable_ptxas_info', 'cuda.enable_debug_info', 'cuda.use_fast_math', 'cuda.cutlass_dir', 'cuda.cutlass_max_profiling_configs', 'cuda.cuda_cxx', 'cuda.cutlass_backend_min_gemm_size', 'cuda.generate_test_runner', 'cuda.cutlass_op_allowlist_regex', 'cuda.cutlass_op_denylist_regex', 'trace.enabled', 'trace.debug_dir', 'trace.debug_log', 'trace.info_log', 'trace.fx_graph', 'trace.fx_graph_transformed', 'trace.ir_pre_fusion', 'trace.ir_post_fusion', 'trace.output_code', 'trace.graph_diagram', 'trace.draw_orig_fx_graph', 'trace.dot_graph_shape', 'trace.log_url_for_graph_xform', 'trace.compile_profile', 'trace.upload_tar', 'trace.log_autotuning_results', '_save_config_ignore', '_cache_config_ignore_prefix']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-311422711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msnapshot_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m \u001b[0;31m# Choose any for long context!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mllama\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mFastLlamaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m    \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastVisionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmistral\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mFastMistralModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpatch_unsloth_smart_gradient_checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0munpatch_unsloth_smart_gradient_checkpointing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m )\n\u001b[0;32m--> 106\u001b[0;31m from unsloth_zoo.loss_utils import (\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mHAS_CUT_CROSS_ENTROPY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mfused_linear_cross_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m compiled_ce_loss_function = torch.compile(\n\u001b[0m\u001b[1;32m    370\u001b[0m     \u001b[0mcompiled_ce_loss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0mfullgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   2109\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inductor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2111\u001b[0;31m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TorchCompileInductorWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2112\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TorchCompileWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, options, dynamic)\u001b[0m\n\u001b[1;32m   1896\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;31m# Stash the compiler_fn to be used for backend match guard.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36mapply_options\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m   1935\u001b[0m             \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1937\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1938\u001b[0m                     \u001b[0;34mf\"Unexpected optimization option {key}, known options are {list(current_config.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unexpected optimization option combo_kernels, known options are ['TYPE_CHECKING', 'debug', 'disable_progress', 'verbose_progress', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache', 'force_disable_caches', 'cpp_wrapper', 'abi_compatible', 'c_shim_version', 'dce', 'static_weight_shapes', 'size_asserts', 'nan_asserts', 'pick_loop_orders', 'inplace_buffers', 'allow_buffer_reuse', 'memory_planning', 'memory_pool', 'benchmark_harness', 'epilogue_fusion', 'epilogue_fusion_first', 'pattern_matcher', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'split_cat_fx_passes', 'efficient_conv_bn_eval_fx_passes', 'is_predispatch', 'group_fusion', 'batch_fusion', 'pre_grad_fusion_options', 'post_grad_fusion_options', 'reorder_for_locality', 'dynamic_scale_rblock', 'force_fuse_int_mm_with_mul', 'use_mixed_mm', 'fx_passes_numeric_check', 'force_mixed_mm', 'reorder_for_compute_comm_overlap', 'reorder_for_compute_comm_overlap_passes', 'estimate_op_runtime', 'intra_node_bw', 'inter_node_bw', 'max_autotune', 'max_autotune_pointwise', 'max_autotune_gemm', 'force_same_precision', 'max_autotune_gemm_backends', 'max_autotune_gemm_search_space', 'autotune_fallback_to_aten', 'unbacked_symint_fallback', 'search_autotune_cache', 'save_args', 'autotune_in_subproc', 'max_autotune_subproc_result_timeout_seconds', 'max_autotune_subproc_graceful_timeout_secon..."
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "max_seq_length = 2048 # Choose any for long context!\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "# Download model and code\n",
        "snapshot_download(\"unsloth/Spark-TTS-0.5B\", local_dir = \"Spark-TTS-0.5B\")\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = f\"Spark-TTS-0.5B/LLM\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = torch.float32, # Spark seems to only work on float32 for now\n",
        "    full_finetuning = True, # We support full finetuning now!\n",
        "    load_in_4bit = False,\n",
        "    #token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:37.365079Z",
          "iopub.status.busy": "2025-03-22T00:51:37.364731Z",
          "iopub.status.idle": "2025-03-22T00:51:44.221612Z",
          "shell.execute_reply": "2025-03-22T00:51:44.220949Z",
          "shell.execute_reply.started": "2025-03-22T00:51:37.365045Z"
        },
        "id": "6bZsfBuZDeCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#LoRA does not work with float32 only works with bfloat16 !!!\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:44.222880Z",
          "iopub.status.busy": "2025-03-22T00:51:44.222617Z",
          "iopub.status.idle": "2025-03-22T00:52:16.516878Z",
          "shell.execute_reply": "2025-03-22T00:52:16.516033Z",
          "shell.execute_reply.started": "2025-03-22T00:51:44.222848Z"
        },
        "id": "LjY75GoYUCB8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:16.518175Z",
          "iopub.status.busy": "2025-03-22T00:52:16.517841Z",
          "iopub.status.idle": "2025-03-22T00:52:35.039329Z",
          "shell.execute_reply": "2025-03-22T00:52:35.038356Z",
          "shell.execute_reply.started": "2025-03-22T00:52:16.518146Z"
        },
        "id": "zK94B-Pfioto",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('Spark-TTS')\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from sparktts.utils.audio import audio_volume_normalize\n",
        "\n",
        "audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n",
        "def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"extract wav2vec2 features\"\"\"\n",
        "\n",
        "        if wavs.shape[0] != 1:\n",
        "\n",
        "             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n",
        "        wav_np = wavs.squeeze(0).cpu().numpy()\n",
        "\n",
        "        processed = audio_tokenizer.processor(\n",
        "            wav_np,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "        input_values = processed.input_values\n",
        "\n",
        "        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n",
        "\n",
        "        model_output = audio_tokenizer.feature_extractor(\n",
        "            input_values,\n",
        "        )\n",
        "\n",
        "\n",
        "        if model_output.hidden_states is None:\n",
        "             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n",
        "\n",
        "        num_layers = len(model_output.hidden_states)\n",
        "        required_layers = [11, 14, 16]\n",
        "        if any(l >= num_layers for l in required_layers):\n",
        "             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n",
        "\n",
        "        feats_mix = (\n",
        "            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n",
        "        ) / 3\n",
        "\n",
        "        return feats_mix\n",
        "def formatting_audio_func(example):\n",
        "    text = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    target_sr = audio_tokenizer.config['sample_rate']\n",
        "\n",
        "    if sampling_rate != target_sr:\n",
        "        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n",
        "        audio_tensor_temp = torch.from_numpy(audio_array).float()\n",
        "        audio_array = resampler(audio_tensor_temp).numpy()\n",
        "\n",
        "    if audio_tokenizer.config[\"volume_normalize\"]:\n",
        "        audio_array = audio_volume_normalize(audio_array)\n",
        "\n",
        "    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n",
        "\n",
        "    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "\n",
        "\n",
        "    feat = extract_wav2vec2_features(audio_tensor)\n",
        "\n",
        "    batch = {\n",
        "\n",
        "        \"wav\": audio_tensor,\n",
        "        \"ref_wav\": ref_wav_tensor,\n",
        "        \"feat\": feat.to(audio_tokenizer.device),\n",
        "    }\n",
        "\n",
        "\n",
        "    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n",
        "\n",
        "    global_tokens = \"\".join(\n",
        "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "    semantic_tokens = \"\".join(\n",
        "        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "\n",
        "    inputs = [\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\",\n",
        "        global_tokens,\n",
        "        \"<|end_global_token|>\",\n",
        "        \"<|start_semantic_token|>\",\n",
        "        semantic_tokens,\n",
        "        \"<|end_semantic_token|>\",\n",
        "        \"<|im_end|>\"\n",
        "    ]\n",
        "    inputs = \"\".join(inputs)\n",
        "    print('Inputs :', inputs)\n",
        "    return {\"text\": inputs}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_audio_func, remove_columns=[\"audio\"])\n",
        "print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n",
        "audio_tokenizer.model.cpu()\n",
        "audio_tokenizer.feature_extractor.cpu()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:09.688959Z",
          "iopub.status.busy": "2025-03-22T00:34:09.688649Z",
          "iopub.status.idle": "2025-03-22T00:34:09.729661Z",
          "shell.execute_reply": "2025-03-22T00:34:09.729001Z",
          "shell.execute_reply.started": "2025-03-22T00:34:09.688939Z"
        },
        "id": "95_Nn-89DhsL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = False, # We're doing full float32 s disable mixed precision\n",
        "        bf16 = False, # We're doing full float32 s disable mixed precision\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:12.049152Z",
          "iopub.status.busy": "2025-03-22T00:34:12.048862Z",
          "iopub.status.idle": "2025-03-22T00:34:14.404349Z",
          "shell.execute_reply": "2025-03-22T00:34:14.403239Z",
          "shell.execute_reply.started": "2025-03-22T00:34:12.049130Z"
        },
        "id": "yqxqAZ7KJ4oL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "input_text = \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\"\n",
        "\n",
        "chosen_voice = None # None for single-speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:35.040842Z",
          "iopub.status.busy": "2025-03-22T00:52:35.040125Z",
          "iopub.status.idle": "2025-03-22T00:52:35.050560Z",
          "shell.execute_reply": "2025-03-22T00:52:35.049663Z",
          "shell.execute_reply.started": "2025-03-22T00:52:35.040818Z"
        },
        "id": "krYI8PrRJ6MX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "FastModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_speech_from_text(\n",
        "    text: str,\n",
        "    temperature: float = 0.8,   # Generation temperature\n",
        "    top_k: int = 50,            # Generation top_k\n",
        "    top_p: float = 1,        # Generation top_p\n",
        "    max_new_audio_tokens: int = 2048, # Max tokens for audio part\n",
        "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates speech audio from text using default voice control parameters.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text input to be converted to speech.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "        top_k (int): Top-k sampling parameter.\n",
        "        top_p (float): Top-p (nucleus) sampling parameter.\n",
        "        max_new_audio_tokens (int): Max number of new tokens to generate (limits audio length).\n",
        "        device (torch.device): Device to run inference on.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Generated waveform as a NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = \"\".join([\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\"\n",
        "    ])\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    print(\"Generating token sequence...\")\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_new_audio_tokens, # Limit generation length\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id, # Stop token\n",
        "        pad_token_id=tokenizer.pad_token_id # Use models pad token id\n",
        "    )\n",
        "    print(\"Token sequence generated.\")\n",
        "\n",
        "\n",
        "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
        "\n",
        "\n",
        "    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n",
        "    # print(f\"\\nGenerated Text (for parsing):\\n{predicts_text}\\n\") # Debugging\n",
        "\n",
        "    # Extract semantic token IDs using regex\n",
        "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
        "    if not semantic_matches:\n",
        "        print(\"Warning: No semantic tokens found in the generated output.\")\n",
        "        # Handle appropriately - perhaps return silence or raise error\n",
        "        return np.array([], dtype=np.float32)\n",
        "\n",
        "    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0) # Add batch dim\n",
        "\n",
        "    # Extract global token IDs using regex (assuming controllable mode also generates these)\n",
        "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
        "    if not global_matches:\n",
        "         print(\"Warning: No global tokens found in the generated output (controllable mode). Might use defaults or fail.\")\n",
        "         pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n",
        "    else:\n",
        "         pred_global_ids = torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0) # Add batch dim\n",
        "\n",
        "    pred_global_ids = pred_global_ids.unsqueeze(0) # Shape becomes (1, 1, N_global)\n",
        "\n",
        "    print(f\"Found {pred_semantic_ids.shape[1]} semantic tokens.\")\n",
        "    print(f\"Found {pred_global_ids.shape[2]} global tokens.\")\n",
        "\n",
        "\n",
        "    # 5. Detokenize using BiCodecTokenizer\n",
        "    print(\"Detokenizing audio tokens...\")\n",
        "    # Ensure audio_tokenizer and its internal model are on the correct device\n",
        "    audio_tokenizer.device = device\n",
        "    audio_tokenizer.model.to(device)\n",
        "    # Squeeze the extra dimension from global tokens as seen in SparkTTS example\n",
        "    wav_np = audio_tokenizer.detokenize(\n",
        "        pred_global_ids.to(device).squeeze(0), # Shape (1, N_global)\n",
        "        pred_semantic_ids.to(device)           # Shape (1, N_semantic)\n",
        "    )\n",
        "    print(\"Detokenization complete.\")\n",
        "\n",
        "    return wav_np\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Generating speech for: '{input_text}'\")\n",
        "    text = f\"{chosen_voice}: \" + input_text if chosen_voice else input_text\n",
        "    generated_waveform = generate_speech_from_text(input_text)\n",
        "\n",
        "    if generated_waveform.size > 0:\n",
        "        import soundfile as sf\n",
        "        output_filename = \"generated_speech_controllable.wav\"\n",
        "        sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n",
        "        sf.write(output_filename, generated_waveform, sample_rate)\n",
        "        print(f\"Audio saved to {output_filename}\")\n",
        "\n",
        "        # Optional: Play in notebook\n",
        "        from IPython.display import Audio, display\n",
        "        display(Audio(generated_waveform, rate=sample_rate))\n",
        "    else:\n",
        "        print(\"Audio generation failed (no tokens found?).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oW-kMpoHZ_M"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2c67f57"
      },
      "source": [
        "!pip install torchcodec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchcodec\n",
        "print(torchcodec.__version__)\n"
      ],
      "metadata": {
        "id": "KFlF8TjYurno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c671795"
      },
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('Spark-TTS')\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from sparktts.utils.audio import audio_volume_normalize\n",
        "\n",
        "audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n",
        "def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"extract wav2vec2 features\"\"\"\n",
        "\n",
        "        if wavs.shape[0] != 1:\n",
        "\n",
        "             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n",
        "        wav_np = wavs.squeeze(0).cpu().numpy()\n",
        "\n",
        "        processed = audio_tokenizer.processor(\n",
        "            wav_np,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "        input_values = processed.input_values\n",
        "\n",
        "        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n",
        "\n",
        "        model_output = audio_tokenizer.feature_extractor(\n",
        "            input_values,\n",
        "        )\n",
        "\n",
        "\n",
        "        if model_output.hidden_states is None:\n",
        "             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n",
        "\n",
        "        num_layers = len(model_output.hidden_states)\n",
        "        required_layers = [11, 14, 16]\n",
        "        if any(l >= num_layers for l in required_layers):\n",
        "             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n",
        "\n",
        "        feats_mix = (\n",
        "            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n",
        "        ) / 3\n",
        "\n",
        "        return feats_mix\n",
        "def formatting_audio_func(example):\n",
        "    text = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    target_sr = audio_tokenizer.config['sample_rate']\n",
        "\n",
        "    if sampling_rate != target_sr:\n",
        "        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n",
        "        audio_tensor_temp = torch.from_numpy(audio_array).float()\n",
        "        audio_array = resampler(audio_tensor_temp).numpy()\n",
        "\n",
        "    if audio_tokenizer.config[\"volume_normalize\"]:\n",
        "        audio_array = audio_volume_normalize(audio_array)\n",
        "\n",
        "    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n",
        "\n",
        "    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "\n",
        "\n",
        "    feat = extract_wav2vec2_features(audio_tensor)\n",
        "\n",
        "    batch = {\n",
        "\n",
        "        \"wav\": audio_tensor,\n",
        "        \"ref_wav\": ref_wav_tensor,\n",
        "        \"feat\": feat.to(audio_tokenizer.device),\n",
        "    }\n",
        "\n",
        "\n",
        "    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n",
        "\n",
        "    global_tokens = \"\".join(\n",
        "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "    semantic_tokens = \"\".join(\n",
        "        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "\n",
        "    inputs = [\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\",\n",
        "        global_tokens,\n",
        "        \"<|end_global_token|>\",\n",
        "        \"<|start_semantic_token|>\",\n",
        "        semantic_tokens,\n",
        "        \"<|end_semantic_token|>\",\n",
        "        \"<|im_end|>\"\n",
        "    ]\n",
        "    inputs = \"\".join(inputs)\n",
        "    print('Inputs :', inputs)\n",
        "    return {\"text\": inputs}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_audio_func, remove_columns=[\"audio\"])\n",
        "print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n",
        "audio_tokenizer.model.cpu()\n",
        "audio_tokenizer.feature_extractor.cpu()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}