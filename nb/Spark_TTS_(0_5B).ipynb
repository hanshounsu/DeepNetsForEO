{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanshounsu/DeepNetsForEO/blob/master/nb/Spark_TTS_(0_5B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xKgNTKiHZ_F"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v0daxqKHZ_H"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDnadAgHHZ_I"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3GyRTGRHZ_I"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dsvDRFgHZ_I",
        "outputId": "f23ff065-2edd-4dab-d135-6f3d86c37d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: xformers==0.0.29.post3 in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Requirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.7.1)\n",
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchcodec as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.5.0\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (838.4 MB)\n",
            "Collecting torchvision==0.20.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.5.0\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "Collecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "Collecting nvidia-curand-cu11==10.3.0.86 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "Collecting nvidia-nccl-cu11==2.21.5 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "Collecting nvidia-nvtx-cu11==11.8.86 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.0)\n",
            "  Using cached https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.0) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.0) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2025.7.1 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.7.1 requires msgspec, which is not installed.\n",
            "unsloth-zoo 2025.7.1 requires tyro, which is not installed.\n",
            "unsloth-zoo 2025.7.1 requires protobuf<4.0.0, but you have protobuf 5.29.5 which is incompatible.\n",
            "xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.5.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.5.0+cu118 torchaudio-2.5.0+cu118 torchvision-0.20.0+cu118 triton-3.1.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchcodec==0.6.0 (from versions: 0.0.0.dev0, 0.0.1, 0.0.2, 0.0.3, 0.1.0, 0.1.1, 0.2.0, 0.2.1, 0.3.0, 0.4.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchcodec==0.6.0\u001b[0m\u001b[31m\n",
            "\u001b[0mfatal: destination path 'Spark-TTS' already exists and is not an empty directory.\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: einx in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from einx) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from einx) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx) (2.4.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->einx) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip uninstall -y torch torchvision torchaudio torchcodec\n",
        "\n",
        "    # 2. PyTorch 2.5.0 + CUDA 11.8 설치 (Colab 환경에 맞춤)\n",
        "    !pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "    # 3. torchcodec 0.6.0 설치 (PyTorch 2.5.x와 호환)\n",
        "    !pip install torchcodec==0.4.0\n",
        "\n",
        "!git clone https://github.com/SparkAudio/Spark-TTS\n",
        "!pip install omegaconf einx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "execution": {
          "iopub.execute_input": "2025-03-22T00:48:54.511089Z",
          "iopub.status.busy": "2025-03-22T00:48:54.510770Z",
          "iopub.status.idle": "2025-03-22T00:51:37.363415Z",
          "shell.execute_reply": "2025-03-22T00:51:37.362696Z",
          "shell.execute_reply.started": "2025-03-22T00:48:54.511053Z"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "c8a5391f-7e14-4db9-e322-68a8f63f3e08",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Unsloth: Pytorch is not installed. Go to https://pytorch.org/.\nWe have some installation instructions on our Github page.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-311422711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msnapshot_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m \u001b[0;31m# Choose any for long context!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Unsloth: Pytorch is not installed. Go to https://pytorch.org/.\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m\"We have some installation instructions on our Github page.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Unsloth: Pytorch is not installed. Go to https://pytorch.org/.\nWe have some installation instructions on our Github page.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "max_seq_length = 2048 # Choose any for long context!\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "# Download model and code\n",
        "snapshot_download(\"unsloth/Spark-TTS-0.5B\", local_dir = \"Spark-TTS-0.5B\")\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = f\"Spark-TTS-0.5B/LLM\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = torch.float32, # Spark seems to only work on float32 for now\n",
        "    full_finetuning = True, # We support full finetuning now!\n",
        "    load_in_4bit = False,\n",
        "    #token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:37.365079Z",
          "iopub.status.busy": "2025-03-22T00:51:37.364731Z",
          "iopub.status.idle": "2025-03-22T00:51:44.221612Z",
          "shell.execute_reply": "2025-03-22T00:51:44.220949Z",
          "shell.execute_reply.started": "2025-03-22T00:51:37.365045Z"
        },
        "id": "6bZsfBuZDeCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#LoRA does not work with float32 only works with bfloat16 !!!\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:44.222880Z",
          "iopub.status.busy": "2025-03-22T00:51:44.222617Z",
          "iopub.status.idle": "2025-03-22T00:52:16.516878Z",
          "shell.execute_reply": "2025-03-22T00:52:16.516033Z",
          "shell.execute_reply.started": "2025-03-22T00:51:44.222848Z"
        },
        "id": "LjY75GoYUCB8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:16.518175Z",
          "iopub.status.busy": "2025-03-22T00:52:16.517841Z",
          "iopub.status.idle": "2025-03-22T00:52:35.039329Z",
          "shell.execute_reply": "2025-03-22T00:52:35.038356Z",
          "shell.execute_reply.started": "2025-03-22T00:52:16.518146Z"
        },
        "id": "zK94B-Pfioto",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('Spark-TTS')\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from sparktts.utils.audio import audio_volume_normalize\n",
        "\n",
        "audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n",
        "def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"extract wav2vec2 features\"\"\"\n",
        "\n",
        "        if wavs.shape[0] != 1:\n",
        "\n",
        "             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n",
        "        wav_np = wavs.squeeze(0).cpu().numpy()\n",
        "\n",
        "        processed = audio_tokenizer.processor(\n",
        "            wav_np,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "        input_values = processed.input_values\n",
        "\n",
        "        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n",
        "\n",
        "        model_output = audio_tokenizer.feature_extractor(\n",
        "            input_values,\n",
        "        )\n",
        "\n",
        "\n",
        "        if model_output.hidden_states is None:\n",
        "             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n",
        "\n",
        "        num_layers = len(model_output.hidden_states)\n",
        "        required_layers = [11, 14, 16]\n",
        "        if any(l >= num_layers for l in required_layers):\n",
        "             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n",
        "\n",
        "        feats_mix = (\n",
        "            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n",
        "        ) / 3\n",
        "\n",
        "        return feats_mix\n",
        "def formatting_audio_func(example):\n",
        "    text = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    target_sr = audio_tokenizer.config['sample_rate']\n",
        "\n",
        "    if sampling_rate != target_sr:\n",
        "        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n",
        "        audio_tensor_temp = torch.from_numpy(audio_array).float()\n",
        "        audio_array = resampler(audio_tensor_temp).numpy()\n",
        "\n",
        "    if audio_tokenizer.config[\"volume_normalize\"]:\n",
        "        audio_array = audio_volume_normalize(audio_array)\n",
        "\n",
        "    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n",
        "\n",
        "    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "\n",
        "\n",
        "    feat = extract_wav2vec2_features(audio_tensor)\n",
        "\n",
        "    batch = {\n",
        "\n",
        "        \"wav\": audio_tensor,\n",
        "        \"ref_wav\": ref_wav_tensor,\n",
        "        \"feat\": feat.to(audio_tokenizer.device),\n",
        "    }\n",
        "\n",
        "\n",
        "    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n",
        "\n",
        "    global_tokens = \"\".join(\n",
        "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "    semantic_tokens = \"\".join(\n",
        "        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "\n",
        "    inputs = [\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\",\n",
        "        global_tokens,\n",
        "        \"<|end_global_token|>\",\n",
        "        \"<|start_semantic_token|>\",\n",
        "        semantic_tokens,\n",
        "        \"<|end_semantic_token|>\",\n",
        "        \"<|im_end|>\"\n",
        "    ]\n",
        "    inputs = \"\".join(inputs)\n",
        "    print('Inputs :', inputs)\n",
        "    return {\"text\": inputs}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_audio_func, remove_columns=[\"audio\"])\n",
        "print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n",
        "audio_tokenizer.model.cpu()\n",
        "audio_tokenizer.feature_extractor.cpu()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:09.688959Z",
          "iopub.status.busy": "2025-03-22T00:34:09.688649Z",
          "iopub.status.idle": "2025-03-22T00:34:09.729661Z",
          "shell.execute_reply": "2025-03-22T00:34:09.729001Z",
          "shell.execute_reply.started": "2025-03-22T00:34:09.688939Z"
        },
        "id": "95_Nn-89DhsL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = False, # We're doing full float32 s disable mixed precision\n",
        "        bf16 = False, # We're doing full float32 s disable mixed precision\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:12.049152Z",
          "iopub.status.busy": "2025-03-22T00:34:12.048862Z",
          "iopub.status.idle": "2025-03-22T00:34:14.404349Z",
          "shell.execute_reply": "2025-03-22T00:34:14.403239Z",
          "shell.execute_reply.started": "2025-03-22T00:34:12.049130Z"
        },
        "id": "yqxqAZ7KJ4oL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "input_text = \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\"\n",
        "\n",
        "chosen_voice = None # None for single-speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:35.040842Z",
          "iopub.status.busy": "2025-03-22T00:52:35.040125Z",
          "iopub.status.idle": "2025-03-22T00:52:35.050560Z",
          "shell.execute_reply": "2025-03-22T00:52:35.049663Z",
          "shell.execute_reply.started": "2025-03-22T00:52:35.040818Z"
        },
        "id": "krYI8PrRJ6MX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "FastModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_speech_from_text(\n",
        "    text: str,\n",
        "    temperature: float = 0.8,   # Generation temperature\n",
        "    top_k: int = 50,            # Generation top_k\n",
        "    top_p: float = 1,        # Generation top_p\n",
        "    max_new_audio_tokens: int = 2048, # Max tokens for audio part\n",
        "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates speech audio from text using default voice control parameters.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text input to be converted to speech.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "        top_k (int): Top-k sampling parameter.\n",
        "        top_p (float): Top-p (nucleus) sampling parameter.\n",
        "        max_new_audio_tokens (int): Max number of new tokens to generate (limits audio length).\n",
        "        device (torch.device): Device to run inference on.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Generated waveform as a NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = \"\".join([\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\"\n",
        "    ])\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    print(\"Generating token sequence...\")\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_new_audio_tokens, # Limit generation length\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id, # Stop token\n",
        "        pad_token_id=tokenizer.pad_token_id # Use models pad token id\n",
        "    )\n",
        "    print(\"Token sequence generated.\")\n",
        "\n",
        "\n",
        "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
        "\n",
        "\n",
        "    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n",
        "    # print(f\"\\nGenerated Text (for parsing):\\n{predicts_text}\\n\") # Debugging\n",
        "\n",
        "    # Extract semantic token IDs using regex\n",
        "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
        "    if not semantic_matches:\n",
        "        print(\"Warning: No semantic tokens found in the generated output.\")\n",
        "        # Handle appropriately - perhaps return silence or raise error\n",
        "        return np.array([], dtype=np.float32)\n",
        "\n",
        "    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0) # Add batch dim\n",
        "\n",
        "    # Extract global token IDs using regex (assuming controllable mode also generates these)\n",
        "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
        "    if not global_matches:\n",
        "         print(\"Warning: No global tokens found in the generated output (controllable mode). Might use defaults or fail.\")\n",
        "         pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n",
        "    else:\n",
        "         pred_global_ids = torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0) # Add batch dim\n",
        "\n",
        "    pred_global_ids = pred_global_ids.unsqueeze(0) # Shape becomes (1, 1, N_global)\n",
        "\n",
        "    print(f\"Found {pred_semantic_ids.shape[1]} semantic tokens.\")\n",
        "    print(f\"Found {pred_global_ids.shape[2]} global tokens.\")\n",
        "\n",
        "\n",
        "    # 5. Detokenize using BiCodecTokenizer\n",
        "    print(\"Detokenizing audio tokens...\")\n",
        "    # Ensure audio_tokenizer and its internal model are on the correct device\n",
        "    audio_tokenizer.device = device\n",
        "    audio_tokenizer.model.to(device)\n",
        "    # Squeeze the extra dimension from global tokens as seen in SparkTTS example\n",
        "    wav_np = audio_tokenizer.detokenize(\n",
        "        pred_global_ids.to(device).squeeze(0), # Shape (1, N_global)\n",
        "        pred_semantic_ids.to(device)           # Shape (1, N_semantic)\n",
        "    )\n",
        "    print(\"Detokenization complete.\")\n",
        "\n",
        "    return wav_np\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Generating speech for: '{input_text}'\")\n",
        "    text = f\"{chosen_voice}: \" + input_text if chosen_voice else input_text\n",
        "    generated_waveform = generate_speech_from_text(input_text)\n",
        "\n",
        "    if generated_waveform.size > 0:\n",
        "        import soundfile as sf\n",
        "        output_filename = \"generated_speech_controllable.wav\"\n",
        "        sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n",
        "        sf.write(output_filename, generated_waveform, sample_rate)\n",
        "        print(f\"Audio saved to {output_filename}\")\n",
        "\n",
        "        # Optional: Play in notebook\n",
        "        from IPython.display import Audio, display\n",
        "        display(Audio(generated_waveform, rate=sample_rate))\n",
        "    else:\n",
        "        print(\"Audio generation failed (no tokens found?).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oW-kMpoHZ_M"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2c67f57"
      },
      "source": [
        "!pip install torchcodec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchcodec\n",
        "print(torchcodec.__version__)\n"
      ],
      "metadata": {
        "id": "KFlF8TjYurno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c671795"
      },
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('Spark-TTS')\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from sparktts.utils.audio import audio_volume_normalize\n",
        "\n",
        "audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n",
        "def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"extract wav2vec2 features\"\"\"\n",
        "\n",
        "        if wavs.shape[0] != 1:\n",
        "\n",
        "             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n",
        "        wav_np = wavs.squeeze(0).cpu().numpy()\n",
        "\n",
        "        processed = audio_tokenizer.processor(\n",
        "            wav_np,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "        input_values = processed.input_values\n",
        "\n",
        "        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n",
        "\n",
        "        model_output = audio_tokenizer.feature_extractor(\n",
        "            input_values,\n",
        "        )\n",
        "\n",
        "\n",
        "        if model_output.hidden_states is None:\n",
        "             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n",
        "\n",
        "        num_layers = len(model_output.hidden_states)\n",
        "        required_layers = [11, 14, 16]\n",
        "        if any(l >= num_layers for l in required_layers):\n",
        "             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n",
        "\n",
        "        feats_mix = (\n",
        "            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n",
        "        ) / 3\n",
        "\n",
        "        return feats_mix\n",
        "def formatting_audio_func(example):\n",
        "    text = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    target_sr = audio_tokenizer.config['sample_rate']\n",
        "\n",
        "    if sampling_rate != target_sr:\n",
        "        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n",
        "        audio_tensor_temp = torch.from_numpy(audio_array).float()\n",
        "        audio_array = resampler(audio_tensor_temp).numpy()\n",
        "\n",
        "    if audio_tokenizer.config[\"volume_normalize\"]:\n",
        "        audio_array = audio_volume_normalize(audio_array)\n",
        "\n",
        "    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n",
        "\n",
        "    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "\n",
        "\n",
        "    feat = extract_wav2vec2_features(audio_tensor)\n",
        "\n",
        "    batch = {\n",
        "\n",
        "        \"wav\": audio_tensor,\n",
        "        \"ref_wav\": ref_wav_tensor,\n",
        "        \"feat\": feat.to(audio_tokenizer.device),\n",
        "    }\n",
        "\n",
        "\n",
        "    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n",
        "\n",
        "    global_tokens = \"\".join(\n",
        "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "    semantic_tokens = \"\".join(\n",
        "        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "\n",
        "    inputs = [\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\",\n",
        "        global_tokens,\n",
        "        \"<|end_global_token|>\",\n",
        "        \"<|start_semantic_token|>\",\n",
        "        semantic_tokens,\n",
        "        \"<|end_semantic_token|>\",\n",
        "        \"<|im_end|>\"\n",
        "    ]\n",
        "    inputs = \"\".join(inputs)\n",
        "    print('Inputs :', inputs)\n",
        "    return {\"text\": inputs}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_audio_func, remove_columns=[\"audio\"])\n",
        "print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n",
        "audio_tokenizer.model.cpu()\n",
        "audio_tokenizer.feature_extractor.cpu()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}