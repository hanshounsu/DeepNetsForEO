{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanshounsu/DeepNetsForEO/blob/master/nb/Spark_TTS_(0_5B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX4zqMxXVz-O"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwnnma5tVz-R"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXzqhFKJVz-R"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR3vEKblVz-R"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89NGZ3NQVz-R",
        "outputId": "5ac47618-69d0-4c6e-b255-f3e1f53a435c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting xformers==0.0.29.post3\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.19.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Collecting cut_cross_entropy\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth_zoo==2025 (from versions: 2024.10.0, 2024.10.1, 2024.10.2, 2024.10.3, 2024.10.4, 2024.10.5, 2024.11.0, 2024.11.1, 2024.11.2, 2024.11.4, 2024.11.5, 2024.11.6, 2024.11.7, 2024.11.8, 2024.12.1, 2024.12.3, 2024.12.4, 2024.12.5, 2024.12.6, 2024.12.7, 2025.1.1, 2025.1.2, 2025.1.3, 2025.1.4, 2025.1.5, 2025.2.1, 2025.2.2, 2025.2.3, 2025.2.4, 2025.2.5, 2025.2.6, 2025.2.7, 2025.3.1, 2025.3.2, 2025.3.3, 2025.3.4, 2025.3.5, 2025.3.6, 2025.3.7, 2025.3.8, 2025.3.9, 2025.3.11, 2025.3.12, 2025.3.13, 2025.3.14, 2025.3.15, 2025.3.16, 2025.3.17, 2025.4.1, 2025.4.2, 2025.4.3, 2025.4.4, 2025.5.1, 2025.5.2, 2025.5.3, 2025.5.4, 2025.5.5, 2025.5.6, 2025.5.7, 2025.5.8, 2025.5.9, 2025.5.10, 2025.5.11, 2025.6.1, 2025.6.2, 2025.6.3, 2025.6.4, 2025.6.5, 2025.6.6, 2025.6.7, 2025.6.8, 2025.7.1, 2025.7.2, 2025.7.3, 2025.7.4)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth_zoo==2025\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo==2025.6.7\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    # !pip uninstall torch torchvision torchaudio\n",
        "    # !pip3 install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\n",
        "\n",
        "!git clone https://github.com/SparkAudio/Spark-TTS\n",
        "!pip install omegaconf einx\n",
        "!pip install torchcodec==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip index versions unsloth_zoo\n",
        "# !pip install unsloth_zoo==2025.6.7"
      ],
      "metadata": {
        "id": "oQyPVJ2AjR7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:48:54.511089Z",
          "iopub.status.busy": "2025-03-22T00:48:54.510770Z",
          "iopub.status.idle": "2025-03-22T00:51:37.363415Z",
          "shell.execute_reply": "2025-03-22T00:51:37.362696Z",
          "shell.execute_reply.started": "2025-03-22T00:48:54.511053Z"
        },
        "id": "QmUBVEnvCDJv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "max_seq_length = 2048 # Choose any for long context!\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "# Download model and code\n",
        "snapshot_download(\"unsloth/Spark-TTS-0.5B\", local_dir = \"Spark-TTS-0.5B\")\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = f\"Spark-TTS-0.5B/LLM\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = torch.float32, # Spark seems to only work on float32 for now\n",
        "    full_finetuning = True, # We support full finetuning now!\n",
        "    load_in_4bit = False,\n",
        "    #token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:37.365079Z",
          "iopub.status.busy": "2025-03-22T00:51:37.364731Z",
          "iopub.status.idle": "2025-03-22T00:51:44.221612Z",
          "shell.execute_reply": "2025-03-22T00:51:44.220949Z",
          "shell.execute_reply.started": "2025-03-22T00:51:37.365045Z"
        },
        "id": "6bZsfBuZDeCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#LoRA does not work with float32 only works with bfloat16 !!!\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:44.222880Z",
          "iopub.status.busy": "2025-03-22T00:51:44.222617Z",
          "iopub.status.idle": "2025-03-22T00:52:16.516878Z",
          "shell.execute_reply": "2025-03-22T00:52:16.516033Z",
          "shell.execute_reply.started": "2025-03-22T00:51:44.222848Z"
        },
        "id": "LjY75GoYUCB8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:16.518175Z",
          "iopub.status.busy": "2025-03-22T00:52:16.517841Z",
          "iopub.status.idle": "2025-03-22T00:52:35.039329Z",
          "shell.execute_reply": "2025-03-22T00:52:35.038356Z",
          "shell.execute_reply.started": "2025-03-22T00:52:16.518146Z"
        },
        "id": "zK94B-Pfioto",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('Spark-TTS')\n",
        "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
        "from sparktts.utils.audio import audio_volume_normalize\n",
        "\n",
        "audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n",
        "def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"extract wav2vec2 features\"\"\"\n",
        "\n",
        "        if wavs.shape[0] != 1:\n",
        "\n",
        "             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n",
        "        wav_np = wavs.squeeze(0).cpu().numpy()\n",
        "\n",
        "        processed = audio_tokenizer.processor(\n",
        "            wav_np,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "        input_values = processed.input_values\n",
        "\n",
        "        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n",
        "\n",
        "        model_output = audio_tokenizer.feature_extractor(\n",
        "            input_values,\n",
        "        )\n",
        "\n",
        "\n",
        "        if model_output.hidden_states is None:\n",
        "             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n",
        "\n",
        "        num_layers = len(model_output.hidden_states)\n",
        "        required_layers = [11, 14, 16]\n",
        "        if any(l >= num_layers for l in required_layers):\n",
        "             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n",
        "\n",
        "        feats_mix = (\n",
        "            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n",
        "        ) / 3\n",
        "\n",
        "        return feats_mix\n",
        "def formatting_audio_func(example):\n",
        "    text = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    target_sr = audio_tokenizer.config['sample_rate']\n",
        "\n",
        "    if sampling_rate != target_sr:\n",
        "        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n",
        "        audio_tensor_temp = torch.from_numpy(audio_array).float()\n",
        "        audio_array = resampler(audio_tensor_temp).numpy()\n",
        "\n",
        "    if audio_tokenizer.config[\"volume_normalize\"]:\n",
        "        audio_array = audio_volume_normalize(audio_array)\n",
        "\n",
        "    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n",
        "\n",
        "    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n",
        "\n",
        "\n",
        "    feat = extract_wav2vec2_features(audio_tensor)\n",
        "\n",
        "    batch = {\n",
        "\n",
        "        \"wav\": audio_tensor,\n",
        "        \"ref_wav\": ref_wav_tensor,\n",
        "        \"feat\": feat.to(audio_tokenizer.device),\n",
        "    }\n",
        "\n",
        "\n",
        "    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n",
        "\n",
        "    global_tokens = \"\".join(\n",
        "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "    semantic_tokens = \"\".join(\n",
        "        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
        "    )\n",
        "\n",
        "    inputs = [\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\",\n",
        "        global_tokens,\n",
        "        \"<|end_global_token|>\",\n",
        "        \"<|start_semantic_token|>\",\n",
        "        semantic_tokens,\n",
        "        \"<|end_semantic_token|>\",\n",
        "        \"<|im_end|>\"\n",
        "    ]\n",
        "    inputs = \"\".join(inputs)\n",
        "    return {\"text\": inputs}\n",
        "\n",
        "\n",
        "dataset = dataset.map(formatting_audio_func, remove_columns=[\"audio\"])\n",
        "print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n",
        "audio_tokenizer.model.cpu()\n",
        "audio_tokenizer.feature_extractor.cpu()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:09.688959Z",
          "iopub.status.busy": "2025-03-22T00:34:09.688649Z",
          "iopub.status.idle": "2025-03-22T00:34:09.729661Z",
          "shell.execute_reply": "2025-03-22T00:34:09.729001Z",
          "shell.execute_reply.started": "2025-03-22T00:34:09.688939Z"
        },
        "id": "95_Nn-89DhsL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = False, # We're doing full float32 s disable mixed precision\n",
        "        bf16 = False, # We're doing full float32 s disable mixed precision\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:12.049152Z",
          "iopub.status.busy": "2025-03-22T00:34:12.048862Z",
          "iopub.status.idle": "2025-03-22T00:34:14.404349Z",
          "shell.execute_reply": "2025-03-22T00:34:14.403239Z",
          "shell.execute_reply.started": "2025-03-22T00:34:12.049130Z"
        },
        "id": "yqxqAZ7KJ4oL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "input_text = \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\"\n",
        "\n",
        "chosen_voice = None # None for single-speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:35.040842Z",
          "iopub.status.busy": "2025-03-22T00:52:35.040125Z",
          "iopub.status.idle": "2025-03-22T00:52:35.050560Z",
          "shell.execute_reply": "2025-03-22T00:52:35.049663Z",
          "shell.execute_reply.started": "2025-03-22T00:52:35.040818Z"
        },
        "id": "krYI8PrRJ6MX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "FastModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_speech_from_text(\n",
        "    text: str,\n",
        "    temperature: float = 0.8,   # Generation temperature\n",
        "    top_k: int = 50,            # Generation top_k\n",
        "    top_p: float = 1,        # Generation top_p\n",
        "    max_new_audio_tokens: int = 2048, # Max tokens for audio part\n",
        "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates speech audio from text using default voice control parameters.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text input to be converted to speech.\n",
        "        temperature (float): Sampling temperature for generation.\n",
        "        top_k (int): Top-k sampling parameter.\n",
        "        top_p (float): Top-p (nucleus) sampling parameter.\n",
        "        max_new_audio_tokens (int): Max number of new tokens to generate (limits audio length).\n",
        "        device (torch.device): Device to run inference on.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Generated waveform as a NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = \"\".join([\n",
        "        \"<|task_tts|>\",\n",
        "        \"<|start_content|>\",\n",
        "        text,\n",
        "        \"<|end_content|>\",\n",
        "        \"<|start_global_token|>\"\n",
        "    ])\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    print(\"Generating token sequence...\")\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_new_audio_tokens, # Limit generation length\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id, # Stop token\n",
        "        pad_token_id=tokenizer.pad_token_id # Use models pad token id\n",
        "    )\n",
        "    print(\"Token sequence generated.\")\n",
        "\n",
        "\n",
        "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
        "\n",
        "\n",
        "    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n",
        "    # print(f\"\\nGenerated Text (for parsing):\\n{predicts_text}\\n\") # Debugging\n",
        "\n",
        "    # Extract semantic token IDs using regex\n",
        "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
        "    if not semantic_matches:\n",
        "        print(\"Warning: No semantic tokens found in the generated output.\")\n",
        "        # Handle appropriately - perhaps return silence or raise error\n",
        "        return np.array([], dtype=np.float32)\n",
        "\n",
        "    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0) # Add batch dim\n",
        "\n",
        "    # Extract global token IDs using regex (assuming controllable mode also generates these)\n",
        "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
        "    if not global_matches:\n",
        "         print(\"Warning: No global tokens found in the generated output (controllable mode). Might use defaults or fail.\")\n",
        "         pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n",
        "    else:\n",
        "         pred_global_ids = torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0) # Add batch dim\n",
        "\n",
        "    pred_global_ids = pred_global_ids.unsqueeze(0) # Shape becomes (1, 1, N_global)\n",
        "\n",
        "    print(f\"Found {pred_semantic_ids.shape[1]} semantic tokens.\")\n",
        "    print(f\"Found {pred_global_ids.shape[2]} global tokens.\")\n",
        "\n",
        "\n",
        "    # 5. Detokenize using BiCodecTokenizer\n",
        "    print(\"Detokenizing audio tokens...\")\n",
        "    # Ensure audio_tokenizer and its internal model are on the correct device\n",
        "    audio_tokenizer.device = device\n",
        "    audio_tokenizer.model.to(device)\n",
        "    # Squeeze the extra dimension from global tokens as seen in SparkTTS example\n",
        "    wav_np = audio_tokenizer.detokenize(\n",
        "        pred_global_ids.to(device).squeeze(0), # Shape (1, N_global)\n",
        "        pred_semantic_ids.to(device)           # Shape (1, N_semantic)\n",
        "    )\n",
        "    print(\"Detokenization complete.\")\n",
        "\n",
        "    return wav_np\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Generating speech for: '{input_text}'\")\n",
        "    text = f\"{chosen_voice}: \" + input_text if chosen_voice else input_text\n",
        "    generated_waveform = generate_speech_from_text(input_text)\n",
        "\n",
        "    if generated_waveform.size > 0:\n",
        "        import soundfile as sf\n",
        "        output_filename = \"generated_speech_controllable.wav\"\n",
        "        sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n",
        "        sf.write(output_filename, generated_waveform, sample_rate)\n",
        "        print(f\"Audio saved to {output_filename}\")\n",
        "\n",
        "        # Optional: Play in notebook\n",
        "        from IPython.display import Audio, display\n",
        "        display(Audio(generated_waveform, rate=sample_rate))\n",
        "    else:\n",
        "        print(\"Audio generation failed (no tokens found?).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-dIz73RVz-W"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}